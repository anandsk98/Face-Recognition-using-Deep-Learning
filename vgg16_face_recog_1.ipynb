{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries as shown below\n",
    "\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-size all the images to this\n",
    "IMAGE_SIZE = [224, 224]#vgg16 input size is 224,224\n",
    "\n",
    "train_path = 'dataset/training_set'\n",
    "valid_path = 'dataset/test_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Vgg 16 library as shown below and add preprocessing layer to the front of VGG\n",
    "# Here we will be using imagenet weights\n",
    "\n",
    "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't train existing weights\n",
    "#very imp as weights are fixed ...it is a state of art algo\n",
    "\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset/training_set\\\\anand', 'dataset/training_set\\\\not_anand']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  # useful for getting number of output classes\n",
    "    #how many folders inside train dataset describes no.of classes\n",
    "folders = glob('dataset/training_set/*')\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our layers - you can add more if you want\n",
    "x = Flatten()(vgg.output)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = Dense(len(folders), activation='softmax')(x)\n",
    "#we cut the last layer of resnet and added 3 categ\n",
    "# create a model object\n",
    "model = Model(inputs=vgg.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 50178     \n",
      "=================================================================\n",
      "Total params: 14,764,866\n",
      "Trainable params: 50,178\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell the model what cost and optimization method to use\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (224, 224),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2/2 [==============================] - 36s 18s/step - loss: 2.2029 - accuracy: 0.4211 - val_loss: 0.7603 - val_accuracy: 0.5526\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 27s 14s/step - loss: 0.2850 - accuracy: 0.8947 - val_loss: 0.4901 - val_accuracy: 0.6579\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 15s 7s/step - loss: 0.4670 - accuracy: 0.7895 - val_loss: 0.2545 - val_accuracy: 0.8684\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 15s 7s/step - loss: 0.2474 - accuracy: 0.8684 - val_loss: 0.0191 - val_accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 16s 8s/step - loss: 0.0237 - accuracy: 1.0000 - val_loss: 0.0435 - val_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 15s 8s/step - loss: 0.0717 - accuracy: 1.0000 - val_loss: 0.0916 - val_accuracy: 0.9737\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 26s 13s/step - loss: 0.0956 - accuracy: 0.9211 - val_loss: 0.0575 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 28s 14s/step - loss: 0.0362 - accuracy: 1.0000 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 27s 14s/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 16s 8s/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 7.2941e-04 - val_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 19s 9s/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 6.8119e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 17s 9s/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 8.9826e-04 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 18s 9s/step - loss: 9.4728e-04 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 28s 14s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 15s 8s/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 15s 7s/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 15s 7s/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 27s 13s/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 15s 7s/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 15s 8s/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 8.7995e-04 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 26s 13s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 6.8641e-04 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 28s 14s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 5.4881e-04 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 17s 9s/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 4.3566e-04 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 28s 14s/step - loss: 1.8654e-04 - accuracy: 1.0000 - val_loss: 3.5819e-04 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 16s 8s/step - loss: 6.6406e-04 - accuracy: 1.0000 - val_loss: 2.9793e-04 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 18s 9s/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 2.3639e-04 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 17s 9s/step - loss: 8.9820e-04 - accuracy: 1.0000 - val_loss: 1.9017e-04 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 21s 10s/step - loss: 8.0272e-04 - accuracy: 1.0000 - val_loss: 1.5509e-04 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 15s 7s/step - loss: 3.6272e-04 - accuracy: 1.0000 - val_loss: 1.3200e-04 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 15s 7s/step - loss: 1.6162e-04 - accuracy: 1.0000 - val_loss: 1.1667e-04 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 15s 8s/step - loss: 1.2328e-04 - accuracy: 1.0000 - val_loss: 1.0620e-04 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 15s 7s/step - loss: 4.2551e-04 - accuracy: 1.0000 - val_loss: 9.8404e-05 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 27s 13s/step - loss: 1.4136e-04 - accuracy: 1.0000 - val_loss: 9.2767e-05 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 25s 13s/step - loss: 8.0693e-04 - accuracy: 1.0000 - val_loss: 8.7513e-05 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 14s 7s/step - loss: 4.1141e-04 - accuracy: 1.0000 - val_loss: 8.3365e-05 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 15s 7s/step - loss: 5.5712e-04 - accuracy: 1.0000 - val_loss: 8.0192e-05 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 15s 7s/step - loss: 1.8410e-04 - accuracy: 1.0000 - val_loss: 7.7784e-05 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 26s 13s/step - loss: 3.1176e-04 - accuracy: 1.0000 - val_loss: 7.5974e-05 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 15s 7s/step - loss: 1.7629e-04 - accuracy: 1.0000 - val_loss: 7.4726e-05 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 25s 13s/step - loss: 1.6752e-04 - accuracy: 1.0000 - val_loss: 7.3851e-05 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 14s 7s/step - loss: 1.8245e-04 - accuracy: 1.0000 - val_loss: 7.3208e-05 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 27s 14s/step - loss: 3.4057e-04 - accuracy: 1.0000 - val_loss: 7.2719e-05 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 15s 7s/step - loss: 3.2718e-04 - accuracy: 1.0000 - val_loss: 7.2305e-05 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 15s 7s/step - loss: 1.0686e-04 - accuracy: 1.0000 - val_loss: 7.1969e-05 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 17s 9s/step - loss: 1.9355e-04 - accuracy: 1.0000 - val_loss: 7.1715e-05 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 30s 15s/step - loss: 3.8230e-04 - accuracy: 1.0000 - val_loss: 7.1477e-05 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 28s 14s/step - loss: 2.4269e-04 - accuracy: 1.0000 - val_loss: 7.1279e-05 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 28s 14s/step - loss: 1.6209e-04 - accuracy: 1.0000 - val_loss: 7.1078e-05 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 26s 13s/step - loss: 1.2728e-04 - accuracy: 1.0000 - val_loss: 7.0906e-05 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 26s 13s/step - loss: 2.9586e-04 - accuracy: 1.0000 - val_loss: 7.0749e-05 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "# Run the cell. It will take some time to execute\n",
    "r = model.fit_generator(\n",
    "  training_set,\n",
    "  validation_data=test_set,\n",
    "  epochs=50,\n",
    "  steps_per_epoch=len(training_set),\n",
    "  validation_steps=len(test_set)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model.save('anand_vgg16_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "cnn= load_model('anand_vgg16_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=224x224 at 0x20381B28CA0>\n",
      "[[1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/9.jpg', target_size = (224, 224))\n",
    "print(test_image)\n",
    "test_image = image.img_to_array(test_image)#predict method expects a 2 d array\n",
    "test_image = np.expand_dims(test_image, axis = 0)#to fit in batch of 32\n",
    "result = cnn.predict(test_image)\n",
    "print(result)\n",
    "#training_set.class_indices#print this to know whether 0=cat or dog \n",
    "if result[0][0] == 1:\n",
    "  prediction = 'anand'\n",
    "else:\n",
    "  prediction = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anand\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
