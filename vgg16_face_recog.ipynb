{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries as shown below\n",
    "\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-size all the images to this\n",
    "IMAGE_SIZE = [224, 224]#vgg16 input size is 224,224\n",
    "\n",
    "train_path = 'dataset/training_set'\n",
    "valid_path = 'dataset/test_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Vgg 16 library as shown below and add preprocessing layer to the front of VGG\n",
    "# Here we will be using imagenet weights\n",
    "\n",
    "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't train existing weights\n",
    "#very imp as weights are fixed ...it is a state of art algo\n",
    "\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset/training_set\\\\anand', 'dataset/training_set\\\\not_anand']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  # useful for getting number of output classes\n",
    "    #how many folders inside train dataset describes no.of classes\n",
    "folders = glob('dataset/training_set/*')\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our layers - you can add more if you want\n",
    "x = Flatten()(vgg.output)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = Dense(1, activation='sigmoid')(x)\n",
    "#we cut the last layer of resnet and added 3 categ\n",
    "# create a model object\n",
    "model = Model(inputs=vgg.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 25089     \n",
      "=================================================================\n",
      "Total params: 14,739,777\n",
      "Trainable params: 25,089\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell the model what cost and optimization method to use\n",
    "model.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (224, 224),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2/2 [==============================] - 33s 17s/step - loss: 0.9672 - accuracy: 0.4474 - val_loss: 0.7624 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 15s 8s/step - loss: 0.5588 - accuracy: 0.6053 - val_loss: 0.2219 - val_accuracy: 0.9737\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 27s 14s/step - loss: 0.4411 - accuracy: 0.7368 - val_loss: 0.3895 - val_accuracy: 0.7895\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 28s 14s/step - loss: 0.3580 - accuracy: 0.8158 - val_loss: 0.1151 - val_accuracy: 0.9737\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 30s 15s/step - loss: 0.0825 - accuracy: 1.0000 - val_loss: 0.0441 - val_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 20s 10s/step - loss: 0.0826 - accuracy: 0.9737 - val_loss: 0.0666 - val_accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 30s 15s/step - loss: 0.0798 - accuracy: 1.0000 - val_loss: 0.0872 - val_accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 27s 14s/step - loss: 0.0896 - accuracy: 1.0000 - val_loss: 0.0656 - val_accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 16s 8s/step - loss: 0.0551 - accuracy: 1.0000 - val_loss: 0.0357 - val_accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 27s 14s/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 28s 14s/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0068 - val_accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 16s 8s/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 27s 13s/step - loss: 0.0274 - accuracy: 0.9737 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 16s 8s/step - loss: 0.0297 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 19s 9s/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 17s 9s/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 32s 16s/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 33s 17s/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 33s 17s/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 34s 17s/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 35s 17s/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 36s 18s/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 36s 18s/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 22s 11s/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 20s 10s/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 16s 8s/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 29s 14s/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 28s 14s/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 32s 16s/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 34s 17s/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 20s 10s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 45s 22s/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 38s 19s/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 22s 11s/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 38s 19s/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 43s 22s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 44s 22s/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 9.7358e-04 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 34s 17s/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 9.4542e-04 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 34s 17s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 9.2068e-04 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 33s 17s/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 8.9759e-04 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 23s 12s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 8.7688e-04 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 40s 20s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 8.5661e-04 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 31s 16s/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 8.3909e-04 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 32s 16s/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 8.2393e-04 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 30s 15s/step - loss: 9.4787e-04 - accuracy: 1.0000 - val_loss: 8.1080e-04 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 17s 8s/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 7.9836e-04 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 16s 8s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 7.8655e-04 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 28s 14s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 7.7456e-04 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 18s 9s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 7.6256e-04 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 17s 8s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 7.4983e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "# Run the cell. It will take some time to execute\n",
    "r = model.fit_generator(\n",
    "  training_set,\n",
    "  validation_data=test_set,\n",
    "  epochs=50,\n",
    "  steps_per_epoch=len(training_set),\n",
    "  validation_steps=len(test_set)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model.save('anand_vgg16_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "cnn= load_model('anand_vgg16_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anand': 0, 'not_anand': 1}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices#print this to know whether 0=cat or dog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=224x224 at 0x286828C1F10>\n",
      "[[2.307903e-20]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/3.jpg', target_size = (224, 224))\n",
    "print(test_image)\n",
    "test_image = image.img_to_array(test_image)#predict method expects a 2 d array\n",
    "test_image = np.expand_dims(test_image, axis = 0)#to fit in batch of 32\n",
    "result = cnn.predict(test_image)\n",
    "print(result)\n",
    "#training_set.class_indices#print this to know whether 0=cat or dog \n",
    "if result[0][0] == 0:\n",
    "  prediction = 'anand'\n",
    "else:\n",
    "  prediction = 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
